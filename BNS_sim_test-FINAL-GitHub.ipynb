{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1364b81",
   "metadata": {},
   "source": [
    "# STARTING TWO CELLS IMPORT LIBRARIES AND DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e323597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ligo.skymap.io\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f85f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ligo.skymap.io\n",
    "import healpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skym = ligo.skymap.io.fits.read_sky_map('farah/allsky/36.fits', distances=True)\n",
    "skymap = hp.ud_grade(skym[0][0], 512)\n",
    "print('Yes')\n",
    "\n",
    "\n",
    "def get_probabilities(skymap, ra, dec, radius=0.35*u.deg):\n",
    "    \"\"\"\n",
    "    Compute the probabilities covered in a grid of ra, dec with radius\n",
    "    given a healpix skymap\n",
    "\n",
    "    Pass the legal RA and Dec lists to this function\n",
    "\n",
    "    Note: this radius is such that for the default sky grid (nside=128),\n",
    "    fields overlap such that the area of the sky becomes 1.83 * 4pi steradians.\n",
    "    As a result, the sum of probabilities will be 1.83\n",
    "    \"\"\"\n",
    "    # fact : int, optional\n",
    "    # Only used when inclusive=True. The overlapping test will be done at\n",
    "    # the resolution fact*nside. For NESTED ordering, fact must be a power of 2, less than 2**30,\n",
    "    # else it can be any positive integer. Default: 4.\n",
    "    fact = 1\n",
    "    nside_skymap = hp.npix2nside(len(skymap))\n",
    "    tile_area = np.pi * radius.to(u.deg).value ** 2\n",
    "    pixel_area = hp.nside2pixarea(nside_skymap, degrees=True)\n",
    "    probabilities = np.zeros(len(ra))\n",
    "    vecs = hp.ang2vec(ra.to(u.deg).value, dec.to(u.deg).value, lonlat=True)\n",
    "    for i in range(len(ra)):\n",
    "        sel_pix = hp.query_disc(nside_skymap, vecs[i], radius.to(\n",
    "            u.rad).value, inclusive=True, fact=fact)\n",
    "        probabilities[i] = np.sum(skymap[sel_pix]) * \\\n",
    "            tile_area / pixel_area / len(sel_pix)\n",
    "        probabilities[i] = np.sum(skymap[sel_pix])/ len(sel_pix)\n",
    "    return probabilities\n",
    "\n",
    "\n",
    "\n",
    "def get_top_tiles(probabilities, frac=0.99):\n",
    "    \"\"\"\n",
    "    probabilities may not add up to 1\n",
    "    return indices of tiles that add up to frac of total\n",
    "    \"\"\"\n",
    "    sortorder = np.argsort(probabilities)\n",
    "    p_cum = np.cumsum(probabilities[sortorder]) / np.sum(probabilities)\n",
    "    startind = np.where(p_cum > 1 - frac)[0][0]\n",
    "    top_tiles = sortorder[startind:]\n",
    "    return np.flip(top_tiles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tiles = pd.read_csv('tiles_GIT_7.csv')\n",
    "ralist, declist = np.array(tiles['RA_Center'])*u.deg, np.array(tiles['DEC_Center'])*u.deg\n",
    "\n",
    "probs = get_probabilities(skymap, np.array([108.756])*u.deg, np.array([22.9])*u.deg)\n",
    "\n",
    "\n",
    "\n",
    "probs = get_probabilities(skymap, ralist, declist)\n",
    "tt = get_top_tiles(probs)\n",
    "len(tt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638cb0d",
   "metadata": {},
   "source": [
    "# THIS CELL CALCULATES MEAN DISTANCE AND AREA FOR A GIVEN MAP. I DID THIS MANUALLY TOWARDS THE END AS SOME MAPS CAUSED PYTHON TO CRASH\n",
    "\n",
    "## SEE COMMENT AFTER FUNCTION DEFINITION - THAT IS WHERE MAP NUMBER IS ENTERED\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = np.load('ns.npy')\n",
    "nlist\n",
    "#df = pd.read_csv('farah/injections.dat', skiprows=0, delimiter=\"\\t\")\n",
    "df = pd.read_csv('farah/injections.csv')\n",
    "np.argmin(df.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_tiles(probabilities, frac=0.99):\n",
    "    \"\"\"\n",
    "    probabilities may not add up to 1\n",
    "    return indices of tiles that add up to frac of total\n",
    "    \"\"\"\n",
    "    sortorder = np.argsort(probabilities)\n",
    "    p_cum = np.cumsum(probabilities[sortorder]) / np.sum(probabilities)\n",
    "    startind = np.where(p_cum > 1 - frac)[0][0]\n",
    "    top_tiles = sortorder[startind:]\n",
    "    return np.flip(top_tiles)\n",
    "\n",
    "\n",
    "\n",
    "#####------------------THE LINE BELOW TELLS MAP NUMBER TO DO CALCULATIONS FOR, HERE NSBH_1500-----------------#####\n",
    "\n",
    "badind = [11, 12, 16, 20, 49, 51, 63, 69, 85, 86,\n",
    "         104, 107, 112, 118, 119, 130, 134,138, 152, 167,171, 180,\n",
    "         214,216,222,230,236,262,280,288,296,\n",
    "         313, 314, 325, 339, 341, 363, 377, 380, 383,\n",
    "         412, 422, 446, 470, 483,\n",
    "         508, 521, 522, 534, 547, 570, 579, 581, 587, 592,\n",
    "         623, 647, 652, 686, 687,\n",
    "         708, 717, 723, 740, 760, 766, 798,\n",
    "         804, 808, 829, 847, 864, 873, 896, 897, 899,\n",
    "         908, 914, 929, 948, 956, 973,\n",
    "         1003, 1004, 1009, 1024, 1034, 1037, 1056, 1076, 1095,\n",
    "         1101, 1111, 1127]\n",
    "\n",
    "ind = 900\n",
    "n = nlist[ind]\n",
    "n = 900\n",
    "#n = 405\n",
    "#n = 3535\n",
    "skym = ligo.skymap.io.fits.read_sky_map('farah/allsky/{}.fits'.format(n), distances=True)\n",
    "plt.figure(figsize=(12,10))\n",
    "hp.mollview(skym[0][1])\n",
    "hp.mollview(skym[0][0])\n",
    "tt = get_top_tiles(skym[0][0])\n",
    "li = len(tt)\n",
    "tt = tt[~np.isinf(skym[0][1][tt])]\n",
    "#print('Distance =',np.sum(skym[0][0][tt]*skym[0][1][tt])/np.sum(skym[0][0][tt]))\n",
    "print(\"Distance =\",df['distance'][np.where(df.simulation_id==n)[0]])\n",
    "skymap = hp.ud_grade(skym[0][0], 512)\n",
    "probs = get_probabilities(skymap, ralist, declist)\n",
    "tt = get_top_tiles(probs, 0.99)\n",
    "print('Num GIT tiles = ',len(tt))\n",
    "tt = get_top_tiles(skymap, 0.99)\n",
    "a = len(tt)*hp.nside2pixarea(512)*3282.8\n",
    "print('Sq deg = ',a)\n",
    "print('Mass2 = ', df.mass2[n])\n",
    "print('n = ',n)\n",
    "\n",
    "####  15, 48, 87, 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28981159",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(df.simulation_id==n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee0209",
   "metadata": {},
   "source": [
    "# AREA CALCULATION END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "skym = ligo.skymap.io.fits.read_sky_map('bns_astro/allsky/47.fits', distances=True)\n",
    "skymap = skym[0][0]\n",
    "\n",
    "np.sqrt(len(skymap)/12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e2102",
   "metadata": {},
   "source": [
    "# FETCH CSV FILES FOR GALAXY CATALOG AND MISSING MASS - DIRECTORY STRUCTURE IS DIFF ON GITHUB, PLS CHANGE ACCORDINGLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55088a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat2 = pd.read_csv('../../new_reduced_galcat.csv')\n",
    "new = pd.read_csv('../../missing_mass.csv')\n",
    "cat2['Dist'] = cat2['DistMpc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7392ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('full.csv')\n",
    "\n",
    "names = d1['num'][304:]\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    skym = ligo.skymap.io.fits.read_sky_map('farah/allsky/{}.fits'.format(name), distances=True)\n",
    "    skymap = hp.ud_grade(skym[0][0], 512)\n",
    "    \n",
    "    #print('\\n\\n\\n',name,'\\n\\n\\n')\n",
    "\n",
    "    tiles = pd.read_csv('tiles_GIT_7.csv')\n",
    "    ralist, declist = np.array(tiles['RA_Center'])*u.deg, np.array(tiles['DEC_Center'])*u.deg\n",
    "    probs = get_probabilities(skymap, ralist, declist)\n",
    "    tt = get_top_tiles(probs, 0.99)\n",
    "    print(name,len(tt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e500582a",
   "metadata": {},
   "source": [
    "# MAIN CELL START\n",
    "\n",
    "## AFTER FINDING GOOD LOCALIZATION MAPS, I USED TO LIST THEM IN NSBH_FULL.CSV AND RUN THE MASS-FILLING ON THOSE MAPS ONLY. \n",
    "\n",
    "## LOOP STATEMENT CAN BE MODIFIED AND EARLIER CELL USED TO INCLUDE AREA CALCULATION + IF STATEMENT TO RUN THE MASS FILLING IF SMALL AREA - then manually checking maps not needed\n",
    "\n",
    "\n",
    "## Please make sure to change 'NSBH' to 'BNS' at a couple of places inside this cell when running for bns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('full.csv')\n",
    "\n",
    "names = d1['num'][0:]\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    skym = ligo.skymap.io.fits.read_sky_map('farah/allsky/{}.fits'.format(name), distances=True)\n",
    "    skymap = hp.ud_grade(skym[0][0], 512)\n",
    "    \n",
    "    print('\\n\\n\\n##############################',name,'#################################\\n\\n\\n')\n",
    "\n",
    "    tiles = pd.read_csv('tiles_GIT_7.csv')\n",
    "    ralist, declist = np.array(tiles['RA_Center'])*u.deg, np.array(tiles['DEC_Center'])*u.deg\n",
    "    probs = get_probabilities(skymap, ralist, declist)\n",
    "    tt = get_top_tiles(probs, 0.99)\n",
    "    len(tt)\n",
    "\n",
    "    distmu = hp.ud_grade(skym[0][1], 512)\n",
    "    distsigma = hp.ud_grade(skym[0][2], 512)\n",
    "    prob2d = hp.ud_grade(skym[0][0], 512)\n",
    "\n",
    "\n",
    "    radius = 0.35*u.deg\n",
    "    fact = 1\n",
    "    nside_skymap = 512\n",
    "\n",
    "    tile_area = np.pi * radius.to(u.deg).value ** 2\n",
    "    pixel_area = hp.nside2pixarea(nside_skymap, degrees=True)\n",
    "\n",
    "    sigma_m = np.zeros(len(tt))\n",
    "\n",
    "    mpcd = np.arange(0,1001,20)\n",
    "\n",
    "\n",
    "    \n",
    "    sigma_m_empty_nodist = np.zeros(len(tt))\n",
    "    sigma_m_only = np.zeros(len(tt))\n",
    "    \n",
    "    #if len(tt)\n",
    "\n",
    "    for h in range(len(tt)):\n",
    "        ra = ralist[tt[h]]\n",
    "        dec = declist[tt[h]]\n",
    "\n",
    "        vecs = hp.ang2vec(ra.to(u.deg).value, dec.to(u.deg).value, lonlat=True)\n",
    "\n",
    "        sel_pix = hp.query_disc(nside_skymap, vecs, radius.to(u.rad).value, inclusive=True, fact=fact)\n",
    "        \n",
    "        m,s = np.median(distmu[sel_pix]), np.median(distsigma[sel_pix])\n",
    "        \n",
    "        currcat = cat2[np.isin(np.array(cat2.PIX_ID_512), sel_pix)][['Dist','Mstar']]\n",
    "        currcat = currcat[(currcat.Dist>=m-3*s) & (currcat.Dist<=m+3*s)]\n",
    "        \n",
    "        summass = np.sum(currcat['Mstar'])\n",
    "        masses3 = summass * np.sum(prob2d[sel_pix])\n",
    "        sigma_m_only[h] = summass\n",
    "        sigma_m_empty_nodist[h] = masses3\n",
    "        \n",
    "        prob2d[sel_pix] = 0.0\n",
    "        print(h)\n",
    "        \n",
    "    df = pd.DataFrame({'Unfilled2D':sigma_m_empty_nodist,'MassOnly_Distcut':sigma_m_only})\n",
    "    df.to_csv('Orderings-2D-GIT/Map_{}.csv'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ece231",
   "metadata": {},
   "outputs": [],
   "source": [
    "   for h in range(len(tt)):\n",
    "        ra = ralist[tt[h]]\n",
    "        dec = declist[tt[h]]\n",
    "\n",
    "        vecs = hp.ang2vec(ra.to(u.deg).value, dec.to(u.deg).value, lonlat=True)\n",
    "\n",
    "        sel_pix = hp.query_disc(nside_skymap, vecs, radius.to(u.rad).value, inclusive=True, fact=fact)\n",
    "        \n",
    "        #m,s = np.median(distmu[sel_pix]), np.median(distsigma[sel_pix])\n",
    "        \n",
    "        currcat = cat2[np.isin(np.array(cat2.PIX_ID_2048), sel_pix)][['Dist','Mstar']]\n",
    "        #currcat = currcat[(currcat.Dist>=m-3*s) & (currcat.Dist<=m+3*s)]\n",
    "        t\n",
    "        masses3 = np.sum(currcat['Mstar']) * np.sum(prob2d[sel_pix])\n",
    "        \n",
    "        sigma_m_empty_nodist[h] = masses3\n",
    "        prob2d[sel_pix] = 0.0\n",
    "        print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc8475",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.read_csv('full.csv')\n",
    "\n",
    "names = d1['num'][304:]\n",
    "\n",
    "for name in names:\n",
    "    \n",
    "    skym = ligo.skymap.io.fits.read_sky_map('farah/allsky/{}.fits'.format(name), distances=True)\n",
    "    skymap = hp.ud_grade(skym[0][0], 512)\n",
    "    \n",
    "    print('\\n\\n\\n##############################',name,'#################################\\n\\n\\n')\n",
    "\n",
    "    tiles = pd.read_csv('tiles_GIT_7.csv')\n",
    "    ralist, declist = np.array(tiles['RA_Center'])*u.deg, np.array(tiles['DEC_Center'])*u.deg\n",
    "    probs = get_probabilities(skymap, ralist, declist)\n",
    "    tt = get_top_tiles(probs, 0.99)\n",
    "    len(tt)\n",
    "\n",
    "    distmu = hp.ud_grade(skym[0][1], 512)\n",
    "    distsigma = hp.ud_grade(skym[0][2], 512)\n",
    "    prob2d = hp.ud_grade(skym[0][0], 512)\n",
    "\n",
    "\n",
    "    radius = 0.35*u.deg\n",
    "    fact = 1\n",
    "    nside_skymap = 512\n",
    "\n",
    "    tile_area = np.pi * radius.to(u.deg).value ** 2\n",
    "    pixel_area = hp.nside2pixarea(nside_skymap, degrees=True)\n",
    "\n",
    "    sigma_m = np.zeros(len(tt))\n",
    "\n",
    "    mpcd = np.arange(0,1001,20)\n",
    "\n",
    "\n",
    "    sigma_m_empty_dist = np.zeros(len(tt))\n",
    "    sigma_m_empty_nodist = np.zeros(len(tt))\n",
    "    sigma_m_nonempty = np.zeros(len(tt))\n",
    "    sigma_m_noprob = np.zeros(len(tt))\n",
    "    sigma_m_noprob_filled = np.zeros(len(tt))\n",
    "    \n",
    "    #if len(tt)\n",
    "\n",
    "    for h in range(len(tt)):\n",
    "        ra = ralist[tt[h]]\n",
    "        dec = declist[tt[h]]\n",
    "\n",
    "        vecs = hp.ang2vec(ra.to(u.deg).value, dec.to(u.deg).value, lonlat=True)\n",
    "\n",
    "        sel_pix = hp.query_disc(nside_skymap, vecs, radius.to(u.rad).value, inclusive=True, fact=fact)\n",
    "\n",
    "        masses1 = np.zeros(len(sel_pix))\n",
    "        masses2 = np.zeros(len(sel_pix))\n",
    "        masses3 = np.zeros(len(sel_pix))\n",
    "        masses4 = np.zeros(len(sel_pix))\n",
    "        masses5 = np.zeros(len(sel_pix))\n",
    "\n",
    "        mpcd = np.arange(0,1001,20)\n",
    "\n",
    "        for j in range(len(sel_pix)):\n",
    "            i = sel_pix[j]\n",
    "            currcat = (cat2[cat2.PIX_ID_512 == i][['Dist','Mstar']])\n",
    "\n",
    "            currcat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            indices = np.arange(0,50,1)\n",
    "            filled = []\n",
    "\n",
    "            distfact = 1/distsigma[i] * np.exp( -(mpcd-distmu[i])**2 / (2*distsigma[i]**2) )\n",
    "\n",
    "            for k in range(len(currcat)):\n",
    "                ind = np.argmin(np.abs(new['distcen'] - currcat['Dist'][k]))\n",
    "                filled.append(ind)\n",
    "\n",
    "            unfilled = np.delete(indices, filled)\n",
    "            masses1[j] = prob2d[i]*(np.sum(currcat['Mstar']*distfact[filled]) + np.sum(new['masstofill'][unfilled]*distfact[unfilled] / (new['num_empty_pixels'][unfilled])))\n",
    "\n",
    "            masses2[j] = prob2d[i]*np.sum(currcat['Mstar']*distfact[filled])\n",
    "\n",
    "            masses3[j] = prob2d[i]*np.sum(currcat['Mstar'])\n",
    "            \n",
    "            masses4[j] = np.sum(currcat['Mstar'])\n",
    "            \n",
    "            masses5[j] = masses4[j] + np.sum(new['masstofill'][unfilled] / (new['num_empty_pixels'][unfilled]))\n",
    "\n",
    "        prob2d[sel_pix] = 0.0\n",
    "\n",
    "        sigma_m_empty_dist[h] = np.sum(masses2)\n",
    "        sigma_m_empty_nodist[h] = np.sum(masses3)\n",
    "        sigma_m_nonempty[h] = np.sum(masses1)\n",
    "        sigma_m_noprob[h] = np.sum(masses4)\n",
    "        sigma_m_noprob_filled[h] = np.sum(masses5)\n",
    "        print(h)\n",
    "        \n",
    "    df = pd.DataFrame({'Filled3D':sigma_m_nonempty, 'Unfilled3D':sigma_m_empty_dist, 'Unfilled2D':sigma_m_empty_nodist, 'NoProb':sigma_m_noprob, 'NoProbFilled':sigma_m_noprob_filled})\n",
    "    df.to_csv('Orderings/Map_{}.csv'.format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924e6aa",
   "metadata": {},
   "source": [
    "# MAIN CELL END. \n",
    "\n",
    "## NEXT FEW CELLS DO THE GLOBAL CALCULATION FOR EACH MAP - CAN LEAVE IT TILL ALL MASS-FILLING ORDERINGS HAVE BEEN RUN, AND I CAN DO IT AFTER NEXT WEEK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec67b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = 0\n",
    "\n",
    "d1 = pd.read_csv('full.csv')\n",
    "ar = []\n",
    "dis = []\n",
    "a1, a2, a3, a4, a5, a6 = [],[],[],[], [],[]\n",
    "ref = pd.read_csv('full2.csv')\n",
    "nobs = 100\n",
    "\n",
    "for n in d1['num'][latest:]:\n",
    "    cs = pd.read_csv('Orderings/Map_{}.csv'.format(n))\n",
    "    sigma_m_nonempty, sigma_m_empty_nodist, sigma_m_empty_dist, sigma_m_noprob = np.array(cs['Filled3D']), np.array(cs['Unfilled2D']), np.array(cs['Unfilled3D']), np.array(cs['NoProb'])\n",
    "    fs = pd.read_csv('Orderings-2D-GIT/Map_{}.csv'.format(n))\n",
    "    sigma_2dmp, monly = np.array(fs['Unfilled2D']), np.array(fs['MassOnly_Distcut'])\n",
    "    #sigma_m_empty_nodist = np.array(pd.read_csv('Orderings-2D/Map_{}.csv'.format(n))['Unfilled2D'])\n",
    "    \n",
    "    order_dist = np.flip(np.argsort(sigma_m_empty_dist)[-nobs:])\n",
    "    order_nodist = np.flip(np.argsort(sigma_m_empty_nodist)[-nobs:])\n",
    "    order_all = np.flip(np.argsort(sigma_m_nonempty)[-nobs:])\n",
    "    order_noprob = np.flip(np.argsort(sigma_m_noprob)[-nobs:])\n",
    "    order_2dmp = np.flip(np.argsort(sigma_2dmp)[-nobs:])\n",
    "    order_mdcut = np.flip(np.argsort(monly)[-nobs:])\n",
    "\n",
    "    fullsum = np.sum(sigma_m_nonempty)\n",
    "    a1.append(np.cumsum((sigma_m_nonempty)[order_nodist])[-1] / fullsum)\n",
    "    a2.append(np.cumsum((sigma_m_nonempty)[order_dist])[-1] / fullsum)\n",
    "    a3.append(np.cumsum((sigma_m_nonempty)[order_all])[-1] / fullsum)\n",
    "    a4.append(np.cumsum((sigma_m_nonempty)[order_noprob])[-1] / fullsum)\n",
    "    a5.append(np.cumsum((sigma_m_nonempty)[order_2dmp])[-1] / fullsum)\n",
    "    a6.append(np.cumsum((sigma_m_nonempty)[order_mdcut])[-1] / fullsum)\n",
    "    print(n)\n",
    "    #print(k, a3[-1], a1[-1], a2[-1])\n",
    "        \n",
    "    ar.append(0)\n",
    "    \n",
    "    dis.append(0)\n",
    "    \n",
    "d1['DegArea'][latest:] = ref['DegArea']\n",
    "d1['Mean_Dist'][latest:] = ref['Mean_Dist']\n",
    "d1['P_3dcat'][latest:] = a2\n",
    "d1['P_2dcat_dcut'][latest:] = a5\n",
    "d1['P_2dcat'][latest:] = a1\n",
    "d1['P_massfill'][latest:] = a3\n",
    "d1['P_onlymass'][latest:] = a4\n",
    "d1['P_massdcut'][latest:] = a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d1 = pd.read_csv('BNS_full.csv')\n",
    "p2d = []\n",
    "\n",
    "for n in d1['num'][latest:]:\n",
    "    file = pd.read_csv('Orderings/Map_{}.csv'.format(n))\n",
    "    #print(len(file))\n",
    "    p2 = np.sum(file.Filled3D[:nobs]) / np.sum(file.Filled3D[:])\n",
    "    p2d.append(p2)\n",
    "p2d    \n",
    "d1['P_2d'][latest:] = p2d\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b978f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.to_csv('Sept/full-GIT100.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(d1.P_massfill < d1.P_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f612e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88233d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.to_csv('BNS_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdc1a5",
   "metadata": {},
   "source": [
    "## End of file writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d3539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
